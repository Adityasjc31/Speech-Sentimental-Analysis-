<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Emotion Recognition</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            margin: 50px;
        }
        #recordButton, #stopButton {
            font-size: 16px;
            padding: 10px;
            margin: 10px;
            cursor: pointer;
        }
        #result {
            margin-top: 20px;
            font-size: 18px;
        }
        canvas {
            margin-top: 20px;
            border: 1px solid #ccc;
        }
    </style>
</head>
<body>
    <h1>Voice Emotion Recognition</h1>
    <p>Click the "Record" button to start recording your voice.</p>

    <button id="recordButton" onclick="startRecording()">Record</button>
    <button id="stopButton" onclick="stopRecording()" disabled>Stop</button>

    <div id="result"></div>
    
    <canvas id="waveformCanvas" width="400" height="200"></canvas>
    

    <script>
        let mediaRecorder;
        let chunks = [];
        let audioContext;
        let canvas, canvasCtx;

        async function startRecording() {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

            mediaRecorder = new MediaRecorder(stream);

            // Set up audio context for waveform
            audioContext = new (window.AudioContext || window.webkitAudioContext)();

            // Set up canvas for waveform
            canvas = document.getElementById('waveformCanvas');
            canvasCtx = canvas.getContext('2d');

            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    chunks.push(event.data);

                    // Process audio for waveform
                    const audioBlob = new Blob(chunks, { type: 'audio/wav' });
                    const audioUrl = URL.createObjectURL(audioBlob);

                    drawWaveform(audioUrl);
                }
            };

            mediaRecorder.onstop = () => {
                const blob = new Blob(chunks, { type: 'audio/wav' });
                const url = URL.createObjectURL(blob);
                analyzeEmotion(url);
                chunks = []; // Reset chunks for the next recording
            };

            mediaRecorder.start();
            document.getElementById('recordButton').disabled = true;
            document.getElementById('stopButton').disabled = false;
        }

        function stopRecording() {
            mediaRecorder.stop();
            document.getElementById('recordButton').disabled = false;
            document.getElementById('stopButton').disabled = true;
        }

        async function drawWaveform(audioUrl) {
            const audioBuffer = await loadAudio(audioUrl);
        
            canvasCtx.clearRect(0, 0, canvas.width, canvas.height);
        
            const data = audioBuffer.getChannelData(0);
            const step = Math.ceil(data.length / canvas.width);
            const amp = canvas.height / 2;
        
            for (let i = 0; i < canvas.width; i++) {
                let min = 1.0;
                let max = -1.0;
                for (let j = 0; j < step; j++) {
                    const value = data[i * step + j];
                    if (value > max) {
                        max = value;
                    }
                    if (value < min) {
                        min = value;
                    }
                }
                canvasCtx.fillStyle = 'black';
                canvasCtx.fillRect(i, (1 + min) * amp, 1, Math.max(1, (max - min) * amp));
            }
        }        

        async function loadAudio(url) {
            const response = await fetch(url);
            const arrayBuffer = await response.arrayBuffer();
            const audioContext = new (window.AudioContext || window.webkitAudioContext)();
            return await audioContext.decodeAudioData(arrayBuffer);
        }        
        
        async function analyzeEmotion(audioUrl) {
            const formData = new FormData();
            formData.append('audio', await fetch(audioUrl).then(res => res.blob()), 'audio.webm');

            const response = await fetch('/upload', {
                method: 'POST',
                body: formData
            });

            const result = await response.json();
            document.getElementById('result').innerText = `Emotion: ${result.result}`;
        }
    </script>
</body>
</html>